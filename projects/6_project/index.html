<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bank Transaction Classifier | Kenneth Hahn's Data Science Portfolio </title> <meta name="author" content="Kenneth E. Hahn"> <meta name="description" content="Dash App for Transaction Category Prediction and Budget Visualization"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hahnkenneth.github.io/projects/6_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Kenneth Hahn's Data Science Portfolio </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">hobbies </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bank Transaction Classifier</h1> <p class="post-description">Dash App for Transaction Category Prediction and Budget Visualization</p> </header> <article> <h2 id="background-and-motivation">Background and Motivation</h2> <p>This project is a machine learning project to be able to predict categories tailored to my specific bank transactions. Once I find the best machine learning model to predict my categories, I also delve into creating a locally hosted Dash app in order to easily upload new data, store the transactions, relabel the categories for said transactions, and retrain my model.</p> <p>The motivation behind this project is to be able to easily create a budget based on how much I am spending on a regular basis. Although many institutions have good budget categorizers, I, like many other individuals, use multiple banks and apps for my finances. As a result, I wanted a centralized location that I could use to aggregate my transaction data and gain insights from it as a whole. Plus, the model would only see my data and so it will be customized for myself, which should allow it to better categorize my transactions.</p> <p>The institutions I use are Bank of America (BofA), American Express (AmEx), and Venmo. Although BofA and AmEx are large banks that have their own personal categorization models, Venmo has very little data and thus it is difficult to know what I spend my money on through that app. We will utilize the categorizations that BofA and AmEx provide as well to hopefully enhance our model and allow it to learn all three institution’s transaction data in aggregation.</p> <h2 id="transaction-storage">Transaction Storage</h2> <p>The initial data storage and processing can be reviewed through the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/clean_transactions.py" rel="external nofollow noopener" target="_blank">clean transactions module</a>. I first downloaded as many transaction statements as I could from my various banks and stored them in the <code class="language-plaintext highlighter-rouge">transactions</code> folder. From here, I created a SQLite database and used the same module to standarize the various instituions’ data. All three of the institutions had similar columns; however, they were all formatted differently and named differently.</p> <p>In order to standardize the transactions, I chose to format the three institutions to be similar to American Express’s data as it included more columns that the other banks did not (such as merchant address, city, state, etc.). We will breifly go over Venmo’s transaction data processing as it required the most changes.</p> <p>First, I checked the NaN columns and dropped several columns that had 100% nan values. I also deleted columns that had the same values for every single transaction.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">calc_nan_percent</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Calculate the % of nan values in each column of a DataFrame</span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># drop amount_(tax), tax_rate because they are all empty strings or no value added
# drop destination and funding_source b/c mostly all null and theres not value added
</span><span class="n">drop_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">amount_(tax)</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tax_rate</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">destination</span><span class="sh">'</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">funding_source</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">destination</span><span class="sh">'</span><span class="p">]</span>

<span class="n">venmo_nan</span> <span class="o">=</span> <span class="nf">calc_nan_percent</span><span class="p">(</span><span class="n">venmo_df</span><span class="p">)</span>

<span class="c1"># drop columns that have only null values + the list above
</span><span class="n">venmo_df</span> <span class="o">=</span> <span class="n">venmo_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">venmo_nan</span><span class="p">[</span><span class="n">venmo_nan</span> <span class="o">==</span> <span class="mi">1</span><span class="p">].</span><span class="n">index</span><span class="p">)</span> <span class="o">+</span> <span class="n">drop_columns</span><span class="p">)</span>

</code></pre></div></div> <p>Then I created a function to alter the amount column to make it suitable as a float instead of a string. Venmo data included many non-numeric characters, like “($1,526.77)” that could not be converted directly into a float value. Also the <code class="language-plaintext highlighter-rouge">amount</code> of the transaction was always positive and the only way to know whether the transaction was income or a charge was to review the <code class="language-plaintext highlighter-rouge">type</code> and <code class="language-plaintext highlighter-rouge">from</code> columns in conjunction. As a result, I made the amount negative if the <code class="language-plaintext highlighter-rouge">type</code> was a ‘Payment’ and it came <code class="language-plaintext highlighter-rouge">from</code> myself to indicate that it was money that I spent. Finally, to make the data more compact, I made any transfers to my bank as part of the <code class="language-plaintext highlighter-rouge">note</code> column instead of the <code class="language-plaintext highlighter-rouge">type</code> column so I could drop the <code class="language-plaintext highlighter-rouge">type</code> column.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">extract_amount</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">regex to extract the amount as a float instead of a string.</span><span class="sh">"""</span>
    <span class="c1"># delete commas, parenthesis and whitespace
</span>    <span class="n">amount</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\$|,|\(|\)| </span><span class="sh">'</span><span class="p">,</span><span class="sh">''</span><span class="p">,</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]))</span>
    
    <span class="c1"># check if I paid the person, then make it negative
</span>    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Payment</span><span class="sh">'</span> <span class="ow">and</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">from</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Kenneth Hahn</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">amount</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="nf">abs</span><span class="p">(</span><span class="n">amount</span><span class="p">)</span>

    <span class="c1"># if it's a transfer to my bank, then make the note the Standard Transfer type (description)
</span>    <span class="n">note</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">note</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Standard Transfer</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">note</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Standard Transfer</span><span class="sh">'</span>

    <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">([</span><span class="n">note</span><span class="p">,</span> <span class="n">amount</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">note</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">])</span>

<span class="n">venmo_df</span><span class="p">[[</span><span class="sh">'</span><span class="s">note</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">venmo_df</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">extract_amount</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

</code></pre></div></div> <p>Finally, I renamed the columns to match the other data and I dropped all the redundant columns that were not used or combined with other features.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># rename columns to match amex data
</span><span class="n">venmo_df</span> <span class="o">=</span> <span class="n">venmo_df</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">datetime</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">,</span>
                                    <span class="sh">'</span><span class="s">note</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">,</span>
                                    <span class="sh">'</span><span class="s">terminal_location</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span>
                                    <span class="p">})</span>

<span class="c1"># convert datetime to date object
</span><span class="n">venmo_df</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">venmo_df</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]).</span><span class="n">dt</span><span class="p">.</span><span class="n">date</span>

<span class="c1"># create new column to match amex data
</span><span class="n">venmo_df</span><span class="p">[</span><span class="sh">'</span><span class="s">extended_details</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">from </span><span class="sh">'</span> <span class="o">+</span> <span class="n">venmo_df</span><span class="p">[</span><span class="sh">'</span><span class="s">from</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> to </span><span class="sh">'</span> <span class="o">+</span> <span class="n">venmo_df</span><span class="p">[</span><span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">]</span> 

<span class="c1"># drop redundant columns
</span><span class="n">venmo_df</span> <span class="o">=</span> <span class="n">venmo_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">from</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">to</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">status</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">id</span><span class="sh">'</span><span class="p">])</span>

</code></pre></div></div> <p>Once all three of the DataFrames were processed and ready for storage, I concatenated all three tables and created a unique id for each of the transactions. This is used as the primary key for the data and it uses the date, amount, details, and source of the transaction (where source is the bank or institution where the transaction comes from). With this primary key, the transaction data is imported into the <code class="language-plaintext highlighter-rouge">transactions.db</code> SQLite database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">############################################################################################
#                      Concatenate and Create Unique IDs                                   #
############################################################################################
</span>
<span class="n">combined_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">boa_df</span><span class="p">,</span> <span class="n">amex_df</span><span class="p">,</span> <span class="n">venmo_df</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">create_transaction_id</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Get unique columns to combine and return a unique id</span><span class="sh">"""</span>
    <span class="n">unique_string</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">extended_details</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">hashlib</span><span class="p">.</span><span class="nf">sha256</span><span class="p">(</span><span class="n">unique_string</span><span class="p">.</span><span class="nf">encode</span><span class="p">()).</span><span class="nf">hexdigest</span><span class="p">()</span>

<span class="n">combined_df</span><span class="p">[</span><span class="sh">'</span><span class="s">transaction_id</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">combined_df</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">create_transaction_id</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">############################################################################################
#                      Insert into transactions Database                                   #
############################################################################################
</span>
<span class="k">def</span> <span class="nf">insert_db</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">conn</span><span class="p">):</span>
    <span class="n">cursor</span> <span class="o">=</span> <span class="n">conn</span><span class="p">.</span><span class="nf">cursor</span><span class="p">()</span>
    <span class="n">insert_query</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
    INSERT OR IGNORE INTO transactions (
        transaction_id,
        date,
        description,
        extended_details,
        amount,
        source,
        merchant,
        address,
        city,
        state,
        zip_code,
        category
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    </span><span class="sh">'''</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="nf">iterrows</span><span class="p">():</span>
        <span class="n">cursor</span><span class="p">.</span><span class="nf">execute</span><span class="p">(</span><span class="n">insert_query</span><span class="p">,</span> <span class="p">(</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">transaction_id</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">description</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">extended_details</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">merchant</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">address</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">state</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">zip_code</span><span class="sh">'</span><span class="p">],</span>
            <span class="n">row</span><span class="p">[</span><span class="sh">'</span><span class="s">category</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">))</span>

    <span class="n">conn</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>

<span class="nf">insert_db</span><span class="p">(</span><span class="n">combined_df</span><span class="p">,</span> <span class="n">conn</span><span class="p">)</span>
<span class="n">conn</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

</code></pre></div></div> <h2 id="feature-engineering">Feature Engineering</h2> <p>Once I had the data stored in a database, I needed to further preprocess to be useable for a model. To review the preprocessing steps thoroughly, please review the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/model_selection_notebook.ipynb" rel="external nofollow noopener" target="_blank">model_selection_notebook</a> for the detailed justification on steps. Also, to view how I made the data preprocessing into a Pipeline for app use, please review the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/custom_preprocessors.py" rel="external nofollow noopener" target="_blank">custom_preprocessors module</a>.</p> <h3 id="target-variable">Target Variable</h3> <p>For the target variable <code class="language-plaintext highlighter-rouge">labels</code>, I one hot encoded the variable using LabelBinarizer, ending up with 19 distinct categories for transactions. By reviewing the class distribution below, I saw that the classes are extremely imbalanced; however, I only had 1047 rows available for training data. As a result, I decided to just leave the classes as is. As we’ll see with the final model, the model still does relatively well overall and some confusion with some of the categories won’t impact my budgeting significantly.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/class_distribution-480.webp 480w,/assets/img/class_distribution-800.webp 800w,/assets/img/class_distribution-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/class_distribution.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Distribution of transaction categories </div> <h3 id="numeric-and-datetime-features">Numeric and Datetime Features</h3> <p>Moving onto the features, I started with processing the <code class="language-plaintext highlighter-rouge">date</code> feature. Because a lot of information can be extracted from the date of the transaction (such as a subscription payment or a weekly payment), I decided to split up the date into multiple features: the day of the week (<code class="language-plaintext highlighter-rouge">dow</code>), the day of the month (<code class="language-plaintext highlighter-rouge">dom</code>), the <code class="language-plaintext highlighter-rouge">month</code>, and the <code class="language-plaintext highlighter-rouge">year</code>. From these new columns, I decided to cyclically encode the day of the week and the month so the model can learn the cyclical nature of the dates to capture any regularly scheduled purchases. For the <code class="language-plaintext highlighter-rouge">dom</code>, I used StandardScaler to normalize the data and for the <code class="language-plaintext highlighter-rouge">year</code> I one hot encoded the years, since there were very few years in the data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># use cyclic encoding to encode the cyclic nature of days of the week and month
</span><span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">dow_sin</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">dow</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">dow_cos</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">dow</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">month_sin</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">month</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">month_cos</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">shuffled_txns_df</span><span class="p">[</span><span class="sh">'</span><span class="s">month</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="mi">12</span><span class="p">)</span>

<span class="c1"># one hot encode the year and source
</span><span class="n">encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">year_encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">shuffled_txns_df</span><span class="p">[[</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]])</span>
<span class="n">year_encoded_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">year_encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">encoder</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">([</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">shuffled_txns_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">shuffled_txns_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">year</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">]),</span> <span class="n">year_encoded_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># split the data 
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_temp</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_temp</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">shuffled_txns_df</span><span class="p">,</span> <span class="n">y_onehot</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_temp</span><span class="p">,</span> <span class="n">y_temp</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Scale 'dom' and 'year' using StandardScaler
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scale_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">dom</span><span class="sh">'</span><span class="p">]</span>

</code></pre></div></div> <p>As for our only numeric feature <code class="language-plaintext highlighter-rouge">amount</code>, I decided to use a RobustScaler instead as the transaction amounts vary widely and the RobustScaler is more robust toward outliers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use RobustScaler to scale and replace the 'amount' column
</span><span class="n">robust_scaler</span> <span class="o">=</span> <span class="nc">RobustScaler</span><span class="p">()</span>
<span class="n">amount_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">robust_scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]]).</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">)</span>
<span class="n">amount_val</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">robust_scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">[[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]]).</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">X_val</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">)</span>
<span class="n">amount_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">robust_scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]]).</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">X_test</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Replace the original 'amount' column with the scaled version
</span><span class="n">train_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">amount_train</span>
<span class="n">val_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">amount_val</span>
<span class="n">test_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">amount</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">amount_test</span>

</code></pre></div></div> <h3 id="categorical-features">Categorical Features</h3> <p>Finally, for categorical variables and text data, I decided to combine nearly all of the columns with string type into one <code class="language-plaintext highlighter-rouge">combined_text</code> column. This is because I was dealing with different institutions and some institutions had more information than others, leading to null values. By combining the text data, I ended up with no missing values. Once combined, I then proceeded to remove the punctuation, excess whitepace, and numeric values. One other decision I made was to remove repeated words. This was because there were many transactions that had the same information from multiple columns. For example, this was my longest string before any processing:</p> <blockquote> <p>air japan air japan japan jp jlgb1b73316 airline/air carrier passenger ticket air japan air japan japan jp carrier : air japan passenger name : kenneth hahn ticket number : jlgb1b departure day : additional info : passenger ticket from : tokyo narita apt to : seoul incheon inte from : seoul incheon inte to : tokyo narita aptforeign spend amount: japanese yen commission amount: currency exchange rate: null air japan air japan japan jp ana narita sky center 3b narita international airport narita 282-0005 travel-airline</p> </blockquote> <p>Which was an flight I purchased from Korea to Japan. There is an excessive number of the word “japan” and other words in the text which I believed didn’t add any value to the information of the transaction. As a result, after processing the text, our longest string went from 568 characters to 272 characters, seen below:</p> <blockquote> <p>air japan jp jlgbb airline carrier passenger ticket name kenneth hahn number departure day additional info from tokyo narita apt to seoul incheon inte aptforeign spend amount japanese yen commission currency exchange rate null ana sky center b international airport travel Length of String: 272</p> </blockquote> <p>Once the text was further processed, I then proceeded to tokenize and embed the text using a pretrained DistilBERT embedding from <a href="https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english" rel="external nofollow noopener" target="_blank">HuggingFace</a>. I used a pretrained embedding because I had very little data to train as well as the fact that DistilBERT was much more compact so I could run this locally. After embedding my data, I ended up with an embedding matrix with 768 additional columns. This concluded all the feature engineering for my model and thus we move onto the model selction portion of this project.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># using distilbert for embeddings
</span><span class="n">model_id</span> <span class="o">=</span> <span class="sh">"</span><span class="s">distilbert-base-uncased-finetuned-sst-2-english</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFAutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">embed_bert</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Output the embedding matrix using pretrained model for the combined_text Series</span><span class="sh">"""</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">.</span><span class="nf">tolist</span><span class="p">(),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="sh">'</span><span class="s">tf</span><span class="sh">'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># global avg pooling
</span>    <span class="k">return</span> <span class="n">embeddings</span>

<span class="c1"># embed the data
</span><span class="n">train_embeddings</span> <span class="o">=</span> <span class="nf">embed_bert</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">combined_text</span><span class="sh">'</span><span class="p">])</span>
<span class="n">val_embeddings</span> <span class="o">=</span> <span class="nf">embed_bert</span><span class="p">(</span><span class="n">val_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">combined_text</span><span class="sh">'</span><span class="p">])</span>
<span class="n">test_embeddings</span> <span class="o">=</span> <span class="nf">embed_bert</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">[</span><span class="sh">'</span><span class="s">combined_text</span><span class="sh">'</span><span class="p">])</span>

</code></pre></div></div> <h2 id="model-selection">Model Selection</h2> <h3 id="model-trials">Model Trials</h3> <p>NOTE: to see the full model implementation, please review the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/model_selection_notebook.ipynb" rel="external nofollow noopener" target="_blank">jupyter notebook</a>. To review the final implementation used for the Dash app (this one uses a Custom Model class to implement the ensemble model), please review the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/train_model.py" rel="external nofollow noopener" target="_blank">train_model module</a>.</p> <p>After feature engineering, I ended up with two different arrays for my inputs: <code class="language-plaintext highlighter-rouge">X_train_other</code> and <code class="language-plaintext highlighter-rouge">X_train_embeddings</code>. <code class="language-plaintext highlighter-rouge">X_train_embeddings</code> is the (n, 768) matrix created from the DistilBERT embeddings, while <code class="language-plaintext highlighter-rouge">X_train_other</code> is all other features, with the text features removed. For most of the models we will concatenate the arrays together for training and prediction; however, for the neural network model, I kept this as separate so I could modify the network differently for the two matrices before concatenating.</p> <p>Initially, I started with various baseline models but as it would turn out, I would find that some of the baseline models do just as well as my final model! For starters, I began with a Majority Class Classifier, which would guess the “dining” category for all transactions. This ended up with <code class="language-plaintext highlighter-rouge">44%</code> training and validation accuracy, as expected (man I need to spend less money on eating out…). Next I tried a Logistic Regression Model from scikit-learn. By default, there is already l2 regularization implemented and once fitting the model, I ended up with a <code class="language-plaintext highlighter-rouge">99%</code> training accuracy and a <code class="language-plaintext highlighter-rouge">92.6%</code> validation accuracy!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create and fit LogisticRegression model (found that it only converged after a lot of iterations)
</span><span class="n">log_regression</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2500</span><span class="p">)</span>
<span class="n">log_regression</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># predict and calculate accuracy for training data
</span><span class="n">log_reg_train_pred</span> <span class="o">=</span> <span class="n">log_regression</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">)</span>
<span class="n">y_train_true</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">log_reg_accuracy_train</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_train_true</span><span class="p">,</span> <span class="n">log_reg_train_pred</span><span class="p">)</span>

</code></pre></div></div> <blockquote> <p>Logistic Regression Training Accuracy: 0.9954254345837146</p> <p>Logistic Regression Validation Accuracy: 0.9258241758241759</p> </blockquote> <p>This was a result that I wasn’t expecting and could have just ended here with these fantastic results; however, I wanted to see how other models faired in comparison. I next used a Random Forest Classifier with 500 estimators, but only received a validation accuracy of <code class="language-plaintext highlighter-rouge">89.2%</code>, which is still great but Logistic Regression still did better. I then tried a Support Vector Classifier with a linear kernel and C=0.09 regularization. Because the data was high-dimensional due to the embedding space, I expected the SVC model to do well and after fitting the model, the support vector machine ended with a training accuracy of <code class="language-plaintext highlighter-rouge">97%</code> and a validation accuracy of <code class="language-plaintext highlighter-rouge">92.6%</code>. The validation accuracy ended up being the same as the Logistic Regression model; however, SVC ended with a slightly lower training accuracy, which could end up meaning that this model is marginally better at generalization compared to Logistic Regression.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># use SVC with a linear kernel (probability=True is used for later in the notebook)
# we saw that logistic regressions with some regularization helped so we'll copy the same process but with SVC
</span><span class="n">svm_model</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="p">.</span><span class="mi">09</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">svm_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># create predictions and calculate accuracy
</span><span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">svm_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">)</span>
<span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">svm_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_val_concat</span><span class="p">)</span>

</code></pre></div></div> <blockquote> <p>SVC Train Accuracy: 0.969807868252516</p> <p>SVC Validation Accuracy: 0.9258241758241759</p> </blockquote> <p>Finally, I wanted to attempt to build a Neural Network with my data. The Neural Network took in the <code class="language-plaintext highlighter-rouge">X_train_other</code> and <code class="language-plaintext highlighter-rouge">X_train_embeddings</code> tensors separately and went through a <code class="language-plaintext highlighter-rouge">reduction_layer</code> to go from 768 features down to 150. The non-embedded features would go through a single Dense layer with 64 ReLU neurons with 10% Dropout regularization. Finally, the two tensors would be concatenated and go through one last Dense layer with 128 ReLU units with 10% Dropout and finally an Output Layer with 19 classes and softmax activation. To train the model, I used an early_stopping callback, Adam optimizer, learning rate of 1e-05 and a batch size of 2. The low batch size was due to my small training dataset and larger batch sizes ended up performing worse. The last note for this model was that I used l2 regularization for all of my Dense Layers with a lambda of 0.01 after some trial and error. This was to prevent overfitting, which ended up providing better validation results as well. See full model below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">embeddings_shape</span><span class="p">,</span> <span class="n">other_features_shape</span><span class="p">,</span> <span class="n">num_categories</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Build a Neural Network that will take an embedding tensor and a tensor with the other features (price, source, etc).
    The model will then process them differently. The embedding tensor will go through a reduction layer to reduce the size to 150 neurons.
    The other_features will go through a Dense layer first. Then they will be concatenated and go through a shallow network to learn.

    Params:
    ------------------------------------------
    embeddings_shape (int): number of features in the embeddings array
    other_features_shape (int): number of features in the other_features array
    num_categories (int): number of classes
    learning_rate (float): the learning rate for the optimizer.

    Returns:
    ------------------------------------------
    model (tf model): a classification model that takes an embedding tensor and a tensor of other useful features to classify bank transactions into distinct categories.
    </span><span class="sh">"""</span>
    <span class="c1"># create input layers
</span>    <span class="n">embeddings_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">embeddings_shape</span><span class="p">,))</span>
    <span class="n">other_features_input</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">other_features_shape</span><span class="p">,))</span>

    <span class="c1"># reduce the size of the embeddings tensor to 150 neurons
</span>    <span class="n">reduction_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))(</span><span class="n">embeddings_input</span><span class="p">)</span>

    <span class="c1"># train the model on just the other_features separately first
</span>    <span class="n">other_features_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))(</span><span class="n">other_features_input</span><span class="p">)</span>
    <span class="n">other_features_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">other_features_layer</span><span class="p">)</span>

    <span class="c1"># concatenate the other_features and the embeddings
</span>    <span class="n">concat_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">reduction_layer</span><span class="p">,</span> <span class="n">other_features_layer</span><span class="p">])</span>

    <span class="c1"># go through one Dense layer with Dropout
</span>    <span class="n">dense_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l2</span><span class="p">(</span><span class="mf">0.01</span><span class="p">))(</span><span class="n">concat_layer</span><span class="p">)</span>
    <span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">dense_layer</span><span class="p">)</span>

    <span class="c1"># output with the num_categories using softmax
</span>    <span class="n">output_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_categories</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>

    <span class="c1"># build the model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">embeddings_input</span><span class="p">,</span> <span class="n">other_features_input</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>

    <span class="c1"># specify the optimizer
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># compile model
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>

</code></pre></div></div> <p>The neural network ended with a <code class="language-plaintext highlighter-rouge">97.6%</code> training accuracy and a <code class="language-plaintext highlighter-rouge">91.5%</code> validation accuracy, which performed marginally worse than the SVC and logistic regression models. All three of these models still performed well overall so instead of having to choose one, I decided to use all three and combine into an ensemble model! The justification behind this was that as I continue to update the transaction data with more transactions, I expect the neural network to perform better with larger datasets, which would only improve the overall model.</p> <h3 id="evaluating-the-models">Evaluating the Models</h3> <p>Because all three models (SVC, Logistic, and Deep Neural Network) can output a probability distribution for the classes, I decided to use a soft voting approach where I would just take the average of the three probability distributions and output the class that has the highest probability mean. This would allow me to modify the ensemble in the future to weight the neural network higher as I gain more data and can evaluate the models individually. The result of this ensemble ended with a <code class="language-plaintext highlighter-rouge">98%</code> training accuracy and a <code class="language-plaintext highlighter-rouge">92.6%</code> validation accuracy. This ended up being comparable to my Logistic and SVC models; however, as suggested I only expect the overall ensemble to improve with more transaction data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># calculate prediction probabilities of logistic regression
</span><span class="n">log_reg_proba_train</span> <span class="o">=</span> <span class="n">log_regression</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">)</span>
<span class="n">log_reg_proba_val</span> <span class="o">=</span> <span class="n">log_regression</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_val_concat</span><span class="p">)</span>

<span class="c1"># calculate prediction probabilities of svc
</span><span class="n">svm_proba_train</span> <span class="o">=</span> <span class="n">svm_model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_train_concat</span><span class="p">)</span>
<span class="n">svm_proba_val</span> <span class="o">=</span> <span class="n">svm_model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_val_concat</span><span class="p">)</span>

<span class="c1"># calculate the prediction probabilities from the deep neural network
</span><span class="n">dnn_proba_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="n">X_train_embeddings</span><span class="p">,</span> <span class="n">X_train_other</span><span class="p">])</span>
<span class="n">dnn_proba_val</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">([</span><span class="n">X_val_embeddings</span><span class="p">,</span> <span class="n">X_val_other</span><span class="p">])</span>

<span class="c1"># take the average of the predictions for training and validation accuracies
# then choose the highest probability class
</span><span class="n">combined_proba_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_reg_proba_train</span> <span class="o">+</span> <span class="n">svm_proba_train</span> <span class="o">+</span> <span class="n">dnn_proba_train</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">combined_pred_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">combined_proba_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">combined_proba_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_reg_proba_val</span> <span class="o">+</span> <span class="n">svm_proba_val</span> <span class="o">+</span> <span class="n">dnn_proba_val</span><span class="p">)</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">combined_pred_val</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">combined_proba_val</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># calculate the accuracies of the ensembled models
</span><span class="n">accuracy_train</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">combined_pred_train</span><span class="p">)</span>
<span class="n">accuracy_val</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_val</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">combined_pred_val</span><span class="p">)</span>

</code></pre></div></div> <blockquote> <p>Voting Classifier Training Accuracy: 0.9807868252516011</p> <p>Voting Classifier Validation Accuracy 0.9258241758241759</p> </blockquote> <p>With this model now chosen, I determined the results of the test set and found that the test accuracy was <code class="language-plaintext highlighter-rouge">90.1%</code>. This showed that the model was not the best at generalization; however, an accuracy above 90% was good enough for my purposes. When reviewing the classification report summary and confusion matrix, I saw that a majority of the losses came from classes that had very fiew data points, such as <code class="language-plaintext highlighter-rouge">fees</code>, <code class="language-plaintext highlighter-rouge">education</code>, <code class="language-plaintext highlighter-rouge">healthcare</code>, <code class="language-plaintext highlighter-rouge">income</code>, etc. and this was expected since the beginning. I am hoping that as I continue to use this app and train further, I will continue to see further gains with my model.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/classification_report-480.webp 480w,/assets/img/classification_report-800.webp 800w,/assets/img/classification_report-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/classification_report.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Printed results of the test classification report. </div> <h2 id="dash-app">Dash App</h2> <p>With the model now determined, I now wanted a way to centralize the data. As a result, I decided to use a locally hosted Dash App in order to visualize my transaction data by category, store any new data into the SQLite database, create predictions from new data, easily label any new data, and train my model to improve its performance over time. For this project page, I will not be going in depth with the coding and I will just be giving a high level overview of the app itself. To review the code, run the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/app.py" rel="external nofollow noopener" target="_blank">app.py module</a> or open the <code class="language-plaintext highlighter-rouge">\pages</code> directory to see the code for the different pages on the app.</p> <p>The app is separated into three main pages: <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/transactions_page.py" rel="external nofollow noopener" target="_blank">transactions_page.py</a>, <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/update_database_page.py" rel="external nofollow noopener" target="_blank">update_database_page.py</a>, and the <a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/update_database_page.py" rel="external nofollow noopener" target="_blank">train_model_page.py</a>:</p> <p><a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/transactions_page.py" rel="external nofollow noopener" target="_blank">transactions_page.py</a> is the default dashboard page, used to visualize and filter my transactions database in a central location.</p> <p><a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/update_database_page.py" rel="external nofollow noopener" target="_blank">update_database_page.py</a> is the page to store any new data into the database, via .csv file uploads. The user then has the option to download that new data, manually label the data for the classes, and upload the labelled data into the database.</p> <p><a href="https://github.com/hahnkenneth/ML_Personal_Budgeter/blob/main/pages/update_database_page.py" rel="external nofollow noopener" target="_blank">train_model_page.py</a> is the final page used to show the current metrics of the model as well as the ability to train a new model. Once the new model is trained, the user can review the updated metrics next to the current metrics and decide whether to replace the model or not.</p> <h3 id="transactions-dashboard">Transactions Dashboard</h3> <p>The transactions page can be split up into four main portions as seen in the image below:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transactions_page-480.webp 480w,/assets/img/transactions_page-800.webp 800w,/assets/img/transactions_page-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/transactions_page.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Printed results of the test classification report. </div> <p>Starting off, let’s go over the filters (number 1. in the main image):</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/transactions_filter-480.webp 480w,/assets/img/transactions_filter-800.webp 800w,/assets/img/transactions_filter-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/transactions_filter.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Filters sidebar layout. </div> <p>The <code class="language-plaintext highlighter-rouge">Aggregation Level</code> filter is used for the time trend and it aggregates the transaction amounts into different levels. The levels are monthly, weekly, daily, or yearly. The <code class="language-plaintext highlighter-rouge">Category Labels Type</code> is to either see the categorical spending based on the model’s predctions (<code class="language-plaintext highlighter-rouge">Predicted Category</code>) or based on my own personal categorization (<code class="language-plaintext highlighter-rouge">True Category</code>). As a note, if I did not label a transaction, it will not show up on the dashboard until I label the data. This is much easier to do for <code class="language-plaintext highlighter-rouge">Predicted Category</code> as we will get to when we move onto the <code class="language-plaintext highlighter-rouge">update_database_page</code>. Next, we move onto the <code class="language-plaintext highlighter-rouge">Category Filter</code> this will allow me to choose certain categories to see further data more in depth. By default, I see all the categories. Finally, the <code class="language-plaintext highlighter-rouge">Choose Date Range</code> filter will allow me to modify the max and min time period to view the data. By default, it chooses the max and min dates in the database and the filter will not allow the user to look at data beyond that range.</p> <p>Moving on to the main content of the page. We first begin with the time trends (labelled as 2. on the main picture), this plot has two different time trends, where the transaction amounts are aggregated based on the <code class="language-plaintext highlighter-rouge">Aggregation Level</code> filter. The top time trend is the amount I spend in a given month while the bottom time trend is the amount I gained in a given time period. Next, labelled as 3. in the main picture, I wanted to visualize the total amount per source. As a result, the bar plot is the sum of all transactions for each institution (BoA, AmEx, and Venmo). There are four categories, because I have both a checking account and a credit account with Bank of America. Finally, the last plot (labelled as 4.), is a simple pie chart to show how much I spent on each category within the given time period.</p> <h3 id="store-data-and-predict-categories">Store Data and Predict Categories</h3> <p>Moving onto the <code class="language-plaintext highlighter-rouge">Update Database</code> page, this page can be viewed in four distinct sections viewed in the picture below (I have blacked out some of the data on the DataTable for privacy purposes on my transaction information):</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/update_database_page-480.webp 480w,/assets/img/update_database_page-800.webp 800w,/assets/img/update_database_page-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/update_database_page.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Layout for the Update Database Page </div> <p>Starting out with the first section, the sidebar is the main location to upload any new data or modify any of the rows of the data. The user will upload .csv files for each of the different banks by either dragging and dropping the file into the respective institution that is labelled or the user can click on the institution to select from their files. Once the data is selected and ready to be stored in the database, the user can then press “Upload Data” which will then either replace any already existing transactions or add new rows, based on the primary key <code class="language-plaintext highlighter-rouge">transaction_id</code>. Once the data is uploaded, the user can then click the <code class="language-plaintext highlighter-rouge">Predict Categories</code> button to load the saved ensemble model and predict any unlabelled categories. By viewing the DataTable in number 2. of the image above, there are two final columns <code class="language-plaintext highlighter-rouge">labels</code> and <code class="language-plaintext highlighter-rouge">predicted_labels</code>. The <code class="language-plaintext highlighter-rouge">predicted_labels</code> are the categories determined by my custom model while the <code class="language-plaintext highlighter-rouge">labels</code> is manually labelled by myself.</p> <p>Moving onto the DataTable, the table displays all transaction data from the transactions database updates whenever the page refreshes. This will allow you to view whether the new data was successfully updated and predicted. Another useful feature with the DataTable is the ability to multi sort the columns by clicking the column names to which the user wants to sort by. Underneath the sort buttons/column names, there is also a filter option where the user can query transactions based on words that they type into there.</p> <p>Finally, sections 3. and 4. is to update the <code class="language-plaintext highlighter-rouge">labels</code> column of the data. Now, you only need to do this if you are looking to retrain your model or want updated accuracy numbers. The <code class="language-plaintext highlighter-rouge">Download Unlabelled Data CSV</code> button (labelled as number 4.) will download any rows where the <code class="language-plaintext highlighter-rouge">labels</code> column is null into a .csv file. The user will then label the data by typing which transaction goes into which category. Once completed, the user will then press the <code class="language-plaintext highlighter-rouge">Upload Labelled Data CSV</code> and select the labelled transaction data. This will then update the database based on the <code class="language-plaintext highlighter-rouge">transaction_id</code>, which the user can review after they refresh the page.</p> <h3 id="train-model">Train Model</h3> <p>Once all the data is now labelled and uploaded, I will want to occasionally train my model and update it based on the results of the new training. This leads us to the Train Model page of the app, where we can review the metrics of the model and choose to train a new model or not.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/train_model_1-480.webp 480w,/assets/img/train_model_1-800.webp 800w,/assets/img/train_model_1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/train_model_1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Train Model Page Layout </div> <p>The sidebar now shows the Current Model Performance in terms of the total accuracy and loss. The user can thus press the <code class="language-plaintext highlighter-rouge">Train New Model</code> button which will go about the process of training a model. Once the model is trained, a new confusion matrix, test accuracy, and test loss will be displayed as seen in the picture above. Also, when the new model is trained a message will appear and the <code class="language-plaintext highlighter-rouge">Replace Current Model</code> button will be enabled (you can see how the button is disabled prior to training in the picture below). The user can then select that button to save the new model in a <code class="language-plaintext highlighter-rouge">\current_model</code> directory, thus replacing it. If the user does not want to replace the model, they simply need to refresh or exit the page.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/train_model_sidebar-480.webp 480w,/assets/img/train_model_sidebar-800.webp 800w,/assets/img/train_model_sidebar-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/train_model_sidebar.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Zoomed in view of the sidebar for the train model page </div> <p>As stated previously, the main content of the page will show a confusion matrix of the current and new models in order to calculate precision and recall. If the user scrolls down further, there will also be a bar plot that shows the distribution of the true categories vs predicted categories for the current and new models. This will show how close the distributions actually are.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/train_model_2-480.webp 480w,/assets/img/train_model_2-800.webp 800w,/assets/img/train_model_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/train_model_2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="graph layout" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Train Model Page Layout once scrolling down </div> <p>Just as a note, the new model that I trained has a low accuracy and a higher loss because I only trained for 5 epochs for my neural network to speed up training for visualization purposes. I have since updated that back to 1000 epochs with an early_stopping callback.</p> <h2 id="conclusion">Conclusion</h2> <p>There is still much more I need to develop with this app. Some further steps I need are as follows:</p> <ul> <li>Need more metrics for the current and new models for better understanding of the performance.</li> <li>Create a loading screen for the train model button in order for the user to know the progress (training the model takes several minutes)</li> <li>Determine if there are comparable performance embeddings that take less compute (currently DistilBert still takes around 2 minutes to embed the text data)</li> <li>Collect more data and update the database/models</li> <li>Make the app nicer and potentially review if this should be hosted somewhere online as opposed to on my local computer</li> </ul> <p>Overall, this was a very fun and useful project to me that showed me a small version of the end to end process for creating a machine learning classification model. I hope to apply this in other applications and increase the complexity of my current one over time!</p> </article> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Kenneth E. Hahn. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"projects",description:"A collection of projects that I have worked on personally or through school.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"A list of repositories found throughout this portfolio",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-resume",title:"resume",description:"Here you can find my resume with all educational degrees, work experiences, and relevant skills. A pdf version can be downloaded on the icon at the top right!",section:"Navigation",handler:()=>{window.location.href="/resume/"}},{id:"nav-hobbies",title:"hobbies",description:"my favorite things to do",section:"Navigation",handler:()=>{window.location.href="/hobbies/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-sustainable-food-delivery-service",title:"Sustainable Food Delivery Service",description:"Neo4j graph databases",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-blood-pressure-risk-factors",title:"Blood Pressure Risk Factors",description:"Linear Regression with R",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-voting-difficulty-and-political-party",title:"Voting Difficulty and Political Party",description:"Hypothesis Testing with R",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-remote-work-and-travelling",title:"Remote Work and Travelling",description:"Data Analytics with Python",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-second-price-auction-simulator",title:"Second Price Auction Simulator",description:"Object Oriented Programming with Python",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-bank-transaction-classifier",title:"Bank Transaction Classifier",description:"Dash App for Transaction Category Prediction and Budget Visualization",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-kepler-exoplanet-detection",title:"Kepler Exoplanet Detection",description:"Binary Classification for Exoplanet Detection",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%61%68%6E%6B%65%6E%6E%65%74%68@%62%65%72%6B%65%6C%65%79.%65%64%75","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/hahnkenneth","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/kenneth-hahn-ab981a149","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/kennethehahn","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/hahnkenneth","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>